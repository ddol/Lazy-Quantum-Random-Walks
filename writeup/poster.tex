\documentclass[a0,portrait]{a0poster}
%\usepackage{alltt}
\usepackage{color}
\usepackage{times}
\usepackage{a0poster}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amssymb,amsmath}
\usepackage{array}
\usepackage{tabularx,tabulary}

\newcommand{\ket}[1]{\left| #1\right\rangle}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\title{Fairness in Lazy Quantum Random Walks}
\author{David F. Dolphin and Michael McGettrick}
\address{Department of Information Technology, National University of Ireland, Galway}
\email{\{d.dolphin1,micheal.mcgettrick\}@nuigalway.ie}

\makeheader


%% Column 1
\begin{center}
\col{
\paragraph{Introduction}
Random walks are a statistical tool, used to study patterns in randomness. They can be applied over a finite space (typically a graph) or an infinite continuum.

Quantum (random) walks are the quantum equivalent of classical random walks. They are studied to observe the statistical properties of quantum systems. These results can aid the design of randomized quantum algorithms, particularly efficiency concerns for those algorithms\cite{Ke:2003}.

Much of the work concerning discrete quantum walks deals with a two state bit's, known as qubits. At each time step in a qubit system the particle must move. Our work looks at three state systems, who's particles are known as qutrits. The particle is not forced to move at each time step, there is a possibility that it can remain in the same location. This possibility to remain in place gives rise to the name "lazy" quantum walks.

\paragraph{Classical Walks}
The most approachable application of a discrete classical random walk is a fair coin toss, the result of which moves a particle left or right on an infinite line. After this experiment has been run a number of times the distance from the origin is recorded. This series of experiments is then run a number of times, and the distances from the origin is recorded each time. When a histogram of these results is plotted we see that the distribution of distances from the origin is approximately normal (Figure~\ref{clas_10k}). As the particle can only land on an odd numbered space after an odd number of steps and an even numbered space after an even number of steps, over a larger number of steps every second value will be zero.

\begin{figure}
\includegraphics[height=110mm]{classic-10k-of-100-itterations.pdf}
\caption{Histogram of final positions, 10,000 iterations of 100-step classical walk.}
\label{clas_10k}
\end{figure}    

This can be generalized to show the probability of the particle being at a certain position after a certain number of steps\cite{Ke:2003}.

\begin{center}
\small
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|c|}
\hline 
& -5 & -4 & -3 & -2 & -1 & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline \hline
\hspace*{\fill} 0 \hspace*{\fill} &  &  &  &  &  & 1 &  &  &  &  & \\
\hline 
1 &  &  &  &  & $\dfrac{ 1 }{ 2 }$ &  & $\dfrac{ 1 }{ 2 }$ &  &  &  & \hspace*{\fill} \\
\hline 
2 &  &  &  & $\dfrac{1}{4}$ &  & $\dfrac{1}{2}$ &  & $\dfrac{1}{4}$ &  &  & \hspace*{\fill} \\
\hline 
3 &  &  &  $\dfrac{1}{8}$ &  & $\dfrac{3}{8}$ &  & $\dfrac{3}{8}$ &  & $\dfrac{1}{8}$  &  & \hspace*{\fill} \\
\hline 
4 &  & \hspace*{\fill} $\dfrac{1}{16}$ \hspace*{\fill} &  & \hspace*{\fill} $\dfrac{1}{4}$ \hspace*{\fill} &  & \hspace*{\fill} $\dfrac{3}{8}$ \hspace*{\fill} &  & \hspace*{\fill} $\dfrac{1}{4}$ \hspace*{\fill} &  & \hspace*{\fill} $\dfrac{1}{16}$ \hspace*{\fill} & \hspace*{\fill} \\
\hline 
5 & \hspace*{\fill} $\dfrac{1}{32}$ \hspace*{\fill} &  & \hspace*{\fill} $\dfrac{5}{32}$ \hspace*{\fill} &  & \hspace*{\fill} $\dfrac{5}{16}$ \hspace*{\fill} &  & \hspace*{\fill} $\dfrac{5}{16}$ \hspace*{\fill} &  & \hspace*{\fill} $\dfrac{5}{32}$ \hspace*{\fill} &  & \hspace*{\fill} $\dfrac{1}{32}$ \hspace*{\fill} \\
\hline 
\end{tabular}
\end{center}

\begin{figure}
\caption{The probability of being at position i after T steps of the classical random walk on the line starting in 0.}
\label{clas_ti}
\end{figure}    

However, on a closed graph the probabilities converge over time. On a 4 position circle, where the particle can move clockwise or counter-clockwise after each timestep, the probabilities converge to $\frac{1}{4}$ for each of the four positions.

We now consider a three state system on a graph. A particle can move left, right or remain stationary at each time step. Figure~\ref{clas2_ti} shows the generalized probability distributions of a lazy classical walk.

\begin{center}
\small
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|c|}
\hline 
& -5 & -4 & -3 & -2 & -1 & 0 & 1 & 2 & 3 & 4 & 5 \\
\hline \hline
\hspace*{\fill} 0 \hspace*{\fill} &  &  &  &  &  & 1 &  &  &  &  & \\
\hline 
1 &  &  &  &  & 
$\dfrac{ 1 }{ 3 }$ &
$\dfrac{ 1 }{ 3 }$ &
$\dfrac{ 1 }{ 3 }$ &  &  &  & 
\hspace*{\fill} \\
\hline 
2 &  &  &  & 
$\dfrac{1}{9}$ & 
$\dfrac{ 2 }{ 9 }$ & 
$\dfrac{1}{3}$ & 
$\dfrac{ 2 }{ 9 }$ & 
$\dfrac{1}{9}$ &  &  & 
\hspace*{\fill} \\
\hline 
3 &  &  &  
$\dfrac{1}{27}$ &  
$\dfrac{ 1 }{ 9 }$ & 
$\dfrac{2}{9}$ &  
$\dfrac{ 2 }{ 27 }$ & 
$\dfrac{2}{9}$ &  
$\dfrac{ 1 }{ 9 }$ & 
$\dfrac{1}{27}$  &  & 
\hspace*{\fill} \\
\hline 
4 &  &
\hspace*{\fill} $\dfrac{1}{81}$ \hspace*{\fill} & 
$\dfrac{ 4 }{ 81 }$ & 
\hspace*{\fill} $\dfrac{10}{81}$ \hspace*{\fill} &  
$\dfrac{ 16 }{ 81 }$ & 
\hspace*{\fill} $\dfrac{19}{81}$ \hspace*{\fill} &  
$\dfrac{ 16 }{ 81 }$ & 
\hspace*{\fill} $\dfrac{10}{81}$ \hspace*{\fill} &  
$\dfrac{ 4 }{ 81 }$ & 
\hspace*{\fill} $\dfrac{1}{81}$ \hspace*{\fill} & 
\hspace*{\fill} \\
\hline 
5 & 
\hspace*{\fill} $\dfrac{1}{243}$ \hspace*{\fill} & 
$\dfrac{ 5 }{ 243 }$ & 
\hspace*{\fill} $\dfrac{5}{81}$ \hspace*{\fill} & 
$\dfrac{ 10 }{ 81 }$ & 
\hspace*{\fill} $\dfrac{5}{27}$ \hspace*{\fill} & 
$\dfrac{ 17 }{ 81 }$ & 
\hspace*{\fill} $\dfrac{5}{27}$ \hspace*{\fill} & 
$\dfrac{ 10 }{ 81 }$ & 
\hspace*{\fill} $\dfrac{5}{81}$ \hspace*{\fill} & 
$\dfrac{ 5 }{ 243 }$ & 
\hspace*{\fill} $\dfrac{1}{243}$ \hspace*{\fill} \\
\hline 
\end{tabular}
\end{center}

\begin{figure}
\caption{The probability of being at position i after T steps of the lazy classical random walk on the line starting in 0.}
\label{clas2_ti}
\end{figure}    

}
%% Column 2
\col{ 
The lazy classical walk has a normal distribution, the introduction of the lazy step removes the odd/even restriction. Like the standard classical walk, the lazy classical walk converges to equal values on a closed graph.

It is possible to vary the lazy bias of a classical walk. A common bias is to choose a probability per iteration of $\frac{1}{2}$ for the lazy move, $\frac{1}{4}$ for the clockwise move, and $\frac{1}{4}$ for the counter-clockwise move. This bias can be modeled as two coin tosses, where one coin signals movement or its absence, the second signals the direction of movement (if the first coin signals movement).

However, we have chosen to give each movement possibility a probability of $\frac{1}{3}$. This is for comparison with the quantum randomization function we will be examining.

\paragraph{Hadamard Gate}

The quantum randomizing function we are examining is known as a Hadamard Gate (also known as a Hadamard Coin). A coin used for a two state (qubit) system is $H_2$ 

\begin{equation}
H_2 = \dfrac{1}{\sqrt{2}} \begin{pmatrix}
  1 & 1 \\
  1 & -1 
\end{pmatrix}
\label{eq:1}
\end{equation}

It is unitary and has been shown\cite{Ke:2003} to be fair. 

The Hadamard gate has been generalized by Marttala\cite{Ma:2007} to $H_m$, where $m$ is the number of states in the system. 

\begin{equation}
\sigma = e^\frac{i2\pi}{m}
\label{eq:2}
\end{equation}

\begin{equation}
H_m = \dfrac{1}{\sqrt{m}} \begin{pmatrix}
  1 & 1 & 1 & \cdots & 1 & 1 \\
  1 & \sigma^{(m-1)} & \sigma^{2(m-1)} & \cdots & \sigma^2 & \sigma \\
  1 & \sigma^{(m-2)} & \sigma^{2(m-2)} & \cdots & \sigma^4 & \sigma^2 \\
  1 & \sigma^{(m-3)} & \sigma^{2(m-3)} & \cdots & \sigma^6 & \sigma^3 \\
  \vdots  & \vdots  & \vdots  & \ddots & \vdots  & \vdots  \\
  1 & \sigma & \sigma^2 & \cdots & \sigma^{(m-2)} & \sigma^{(m-1)} \\
\end{pmatrix}
\label{eq:3}
\end{equation}

As we are interested in a lazy walk we want a three state gate $H_3$, where the three states represent a left movement, a right movement, and no movement.

\begin{equation}
H_3 = \dfrac{1}{\sqrt{3}} \begin{pmatrix}
  1 & 1 & 1 \\
  1 & e^{-\frac{2}{3}i\pi} & e^{\frac{2}{3}i\pi} \\
  1 & e^{\frac{2}{3}i\pi} & e^{-\frac{2}{3}i\pi}
\end{pmatrix}
\label{eq:4}
\end{equation}

\begin{equation}
H_3 = \dfrac{1}{\sqrt{3}} \begin{pmatrix}
  1 & 1 & 1 \\
  1 & -(-1)^{\frac{1}{3}} & (-1)^{\frac{2}{3}} \\
  1 & (-1)^{\frac{2}{3}} & -(-1)^{\frac{1}{3}}
\end{pmatrix}
\label{eq:5}
\end{equation}

We are satisfied that $H_3$ is unitary as 

\begin{equation}
H_{3}H_{3}^{\dagger} = I_3
\label{eq:6}
\end{equation}

\paragraph{Quantum Walks on a Graph}

A Lazy One Dimensional Discrete Quantum Walks takes place 
on the state space spanned by vectors
\begin{equation}
\ket{n,p}
\label{eq:7}
\end{equation}
where $n\in Z$ (the integers)
and $p\in \{0,1,2\}$ is a three-state variable. $n$ represents the position a particle on a walk and is the walks classical component. $p$ is the quantum component and is typically a two-state spin. One step of the walk is given by the transitions
\begin{eqnarray}
\ket{n,0} &\longrightarrow a\ket{n,0} + b\ket{n+1,1} + c\ket{n-1,2}\\
\ket{n,1} &\longrightarrow d\ket{n,0} + e\ket{n+1,1} + f\ket{n-1,2}\\
\ket{n,2} &\longrightarrow g\ket{n,0} + h\ket{n+1,1} + i\ket{n-1,2}
\end{eqnarray} 
where 
\begin{equation}
\begin{pmatrix}
  a & b & c \\
  d & e & f \\
  g & h & i 
\end{pmatrix} = H_3
\end{equation}
\cite{Ke:2003,Mc:2010}.

We use a non-parametric estimate of the spectral density and thus avoid the assumptions implicit in parametric approaches. An estimate of the spectral density is obtained by smoothing the
periodogram on log scale \cite{wahba:1980}, $\log(I(\omega))$, to
ensure that the spectral density estimate is positive everywhere. A smoothing spline \cite{silverman:1985} was chosen to
smooth the log-periodogram. The spline is given more freedom at low frequencies to accommodate the effect of the detrending.
To ensure the spectral density of the noise is estimated independently
of any response to the stimulus, the periodogram ordinates at the
fundamental frequency of stimulation and its first two harmonics are
not included in this procedure. Also, a small amount of spatial smoothing is applied to the spectral density estimates. Figure~\ref{periodogram} shows the spectral density estimates before (thick line) and after (dotted line) spatial smoothing.  


\paragraph{Testing for a response to the stimulus}

The spectral density estimate provides us with a baseline against
which to test for significant departures from the underlying process.
From (I),
\begin{equation}
  \frac{I(\omega_j)}{f(\omega_j)} \sim E
\label{eq:13}
\end{equation}
Substituting $g(\omega_j)$ for $f(\omega_j)$, we define the ratio
statistic, $R_j$, as
\begin{equation}
  R_j = \frac{I(\omega_j)}{g(\omega_j)},\qquad\qquad \omega_j=j/\delta
  n
\label{eq:14}
\end{equation}
By calculating the statistic $R_j$ at the fundamental frequency of
activation, $\omega_c$, we obtain a test statistic, $R_c$, for
significant activation. Large values of $R_c$ indicate a large effect at the fundamental
frequency. All of the statistics $R_j$, apart from $j=0$ and $n/2$, will have the same distribution as $R_c$ and thus can be used as a benchmark against which to compare the theoretical distribution of $R_c$, at negligible computational cost. Thus the method is effectively self-calibrating.

\begin{figure}
\includegraphics[height=90mm,viewport=25 169 579
    609]{jm_fig3.eps}
\includegraphics[height=90mm,viewport=25 169 579
    609]{jm_fig2.eps}
  \caption{Trend Removal:(left) Obvious non-linear trend (right) In an area of activation}
\label{trends}
\end{figure}


}
%% Column 3
\col{
\paragraph{Results}
We have compared our method to an implementation of the AR(1) approach of \cite{Bullmore:1996} in which we apply our own non-linear detrending. Figure~\ref{vis} shows thresholded $p$-value maps of the same slice from a periodic visual stimulation dataset. The AR(1) approach shows significantly more false-positive activation in areas away from the visual cortex. We also applied the two approaches to a null dataset, assuming a periodic stimulus. The distribution of each statistic was compared to its theoretical form using $PP$-plots shown if figure~\ref{pp.plots}. The left plot shows the $PP$-plot of the $R_c$ statistic (thick line) and the $PP$-plot for $R_j$ statistics at a range of higher frequencies (dotted line). The agreement of the dotted line with the theoretical (straight line at $45^o$) validates the use of the theoretical distribution when using our method. For the AR(1) approach shown in the right plot the theoretical is not an adequate fit and a randomisation experiment will be needed to correct for this. 
\paragraph{High frequency artefacts}
In some datasets we have found high frequency artefacts that occur in narrow bands and have been attributed to Nyquist ghosting(figure~\ref{ghost} (right)). We have detected these artefacts using our values of $R_j$ at high frequencies (figure~\ref{ghost} (left) shows a thresholded $R_{88}$ image).
We have found that parametric-time domain models may be extremely susceptible to these artefacts whereas non-parametric spectral density estimation will be resistant. Three methods were applied to a voxel in an area exhibiting the high-frequency artefact. Method I: The AR(1) approach described above, Method II: As Method I but with high frequency artefact removed, Method III: Our approach. Comparing Method I and II, in the table below, we see that the estimated parameters are significantly different after the artefact has been removed, which results in a misleading statistic. Even after removal, there is no guarantee that the AR(1) model is flexible enough and this results in the difference between Methods II and III.
\begin{center}
\small
\begin{tabular}{|l||c|c|c|c|c|}
\hline 
& AR(1) coefficient & $\widehat{\sigma^2}$ & Numerator & Denominator & Ratio Statistic\\
\hline \hline
Method I & $-$0.0027 & 15765 & 591.88 & 315.29 & 1.877\\
\hline
Method II & 0.2839 & 11276 & 624.98 & 419.46 & 1.490\\
\hline 
Method III& --- & --- & 689.30 & 513.88 & 1.341\\
\hline 
\end{tabular}
\end{center}
\paragraph{Conclusion and extensions}
Non-parametric spectral estimation is shown to be an accurate and self-calibrating approach for analyzing periodic designs. The method makes few assumptions and is resistant to high-frequency artefacts whereas parametric time-domain approaches may be susceptible to these artefacts and biased by the assumptions they make on the form of the spectral density. The method can be easily extended to handle non-periodic event related designs and initial results are extremely promising.
\paragraph{Acknowledgements} We are grateful to Dr Stephen Smith (fMRIB Centre, Oxford) for
advice and datasets.

\references
\bibliography{jm.ref}
}
\end{center}


\makefooter

\end{document}





